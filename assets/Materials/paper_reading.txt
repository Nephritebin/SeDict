For generating more effective training samples, an ideal external force sampler is supposed to affect
the policy to the extent that the agent experiences an obvious performance drop but is still able to
recover from the disturbance, which guarantees not only the training feasibility but the weakness of
the policy is attacked precisely. To this end, we introduce a disturber network conditioned on the
current states of the robot to generate adaptive external forces. Compared to the actor that aims to
maximize the cumulative discounted overall reward, the disturber is modeled as a separate learnable
module to maximize the cumulative discounted error between the task reward and its oracle, i.e.,
“cost” in each iteration. To ensure stable optimization between the actor and the disturber, we
implement an additional learning objective derived from the constraint inspired by the classical $H_\infty$
theory [25, 26, 27], which mandates the bound of the ratio between the cost and the intensity of
\begin{equation}
a= 100
\end{equation}
external forces generated by the disturber. Following this constraint, we naturally derive an upper
bound for the cost function with respect to a certain intensity of external forces, which is equivalent
to a performance lower bound for the actor with a theoretical guarantee.

We train our method in Isaac Gym simulator [28] and utilize dual gradient descent method [29] for
joint optimization. We evaluate our locomotion policy by comparing it against baseline approaches
in terms of their command-tracking ability under various types of disturbances and terrains. We also
train policies with baseline methods and our method in the non-stationary bipedal walking setting
and measure their abilities to resist collision. In all evaluations, our method outperforms the baseline
method, suggesting the effectiveness and superiority of our method. We deploy the learned policy
on Unitree Aliengo robot and Unitree A1 robot in real-world settings. As shown in Fig. 1, the
robot manages to traverse planes, slopes, stairs, high platforms, and greasy surfaces whether
the external force is applied to the trunk or legs. The robot can even walk with its hind legs
while withstanding the impact from heavy objects.